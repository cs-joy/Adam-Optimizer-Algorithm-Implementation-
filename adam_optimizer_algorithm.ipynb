{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install labml-nn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmXQIelnpFxk",
        "outputId": "2bd0395a-b697-4d37-be2b-e382483a867d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting labml-nn\n",
            "  Downloading labml_nn-0.4.136-py3-none-any.whl (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.0/435.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting labml==0.4.168 (from labml-nn)\n",
            "  Downloading labml-0.4.168-py3-none-any.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.0/131.0 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting labml-helpers==0.4.89 (from labml-nn)\n",
            "  Downloading labml_helpers-0.4.89-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from labml-nn) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (from labml-nn) (0.17.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from labml-nn) (0.17.1+cu121)\n",
            "Collecting einops (from labml-nn)\n",
            "  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from labml-nn) (1.25.2)\n",
            "Collecting fairscale (from labml-nn)\n",
            "  Downloading fairscale-0.4.13.tar.gz (266 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.3/266.3 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gitpython (from labml==0.4.168->labml-nn)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from labml==0.4.168->labml-nn) (6.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->labml-nn) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->labml-nn) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->labml-nn) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->labml-nn) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->labml-nn) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->labml-nn) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->labml-nn)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->labml-nn)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->labml-nn)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->labml-nn)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->labml-nn)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->labml-nn)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->labml-nn)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->labml-nn)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->labml-nn)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch->labml-nn)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->labml-nn)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->labml-nn) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->labml-nn)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext->labml-nn) (4.66.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext->labml-nn) (2.31.0)\n",
            "Requirement already satisfied: torchdata==0.7.1 in /usr/local/lib/python3.10/dist-packages (from torchtext->labml-nn) (0.7.1)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.1->torchtext->labml-nn) (2.0.7)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->labml-nn) (9.4.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython->labml==0.4.168->labml-nn)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->labml-nn) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext->labml-nn) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext->labml-nn) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext->labml-nn) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->labml-nn) (1.3.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython->labml==0.4.168->labml-nn)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: fairscale\n",
            "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.13-py3-none-any.whl size=332108 sha256=33a31da48697ef22cf0706ed67b1921cf7c5ead2a3354991c3d2487b124401e7\n",
            "  Stored in directory: /root/.cache/pip/wheels/78/a4/c0/fb0a7ef03cff161611c3fa40c6cf898f76e58ec421b88e8cb3\n",
            "Successfully built fairscale\n",
            "Installing collected packages: smmap, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, einops, nvidia-cusparse-cu12, nvidia-cudnn-cu12, gitdb, nvidia-cusolver-cu12, gitpython, labml, labml-helpers, fairscale, labml-nn\n",
            "Successfully installed einops-0.8.0 fairscale-0.4.13 gitdb-4.0.11 gitpython-3.1.43 labml-0.4.168 labml-helpers-0.4.89 labml-nn-0.4.136 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 smmap-5.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "w-_at2DtnHz0"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from typing import Dict, Any, Tuple, Optional\n",
        "\n",
        "import torch\n",
        "from labml import tracker\n",
        "from torch import nn\n",
        "\n",
        "from labml_nn.optimizers import GenericAdaptiveOptimizer, WeightDecay"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* `params` is the list of parameters\n",
        "* `lr` is the learning rate α\n",
        "* `betas` is a tuple of (β1​, β2​)\n",
        "* `eps` is ϵ^ or ϵ based on optimized_update\n",
        "* `weight_decay` is an instance of class WeightDecay defined in `__init__()`\n",
        "* `optimized_update` is a flag whether to optimize the bias correction of the second moment by doing it after adding ϵ\n",
        "* `defaults` is a dictionary of default for group values. This is useful when you want to extend the class ``Adam`` .\n"
      ],
      "metadata": {
        "id": "W5Qfq-elg8We"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Adam(GenericAdaptiveOptimizer):\n",
        "  # initialize the optimizer\n",
        "  def __init__(self, params,\n",
        "               lr: float = 1e-3, betas: Tuple[float, float] = (0.0, 0.999),\n",
        "               eps: float = 1e-16,\n",
        "               weight_decay: WeightDecay = WeightDecay(),\n",
        "               optimized_update: bool = True,\n",
        "               defaults: Optional[Dict[str, Any]] = None):\n",
        "    defaults = {} if defaults is None else defaults\n",
        "    defaults.update(weight_decay.defaults())\n",
        "    super().__init__(params, defaults, lr, betas, eps)\n",
        "\n",
        "    self.weight_decay = weight_decay\n",
        "    self.optimized_update = optimized_update\n",
        "\n",
        "\n",
        "  # initialize the parameter state\n",
        "                       # `state` is the optimizer state of the parameter (tensor)\n",
        "                                              # `group` stores optimizer attributes of the parameter group\n",
        "                                                                     # `param` is the parameter tensor θ_t−1\n",
        "  def init_state(self, state: Dict[str, any], group: Dict[str, any], param: nn.Parameter):\n",
        "    state['step'] = 0\n",
        "    state[éxp_avg] = torch.zeros_like(param, memory_format=torch.preserve_format)\n",
        "    state[éxp_avg_sq] = torch.zeros_like(param, memory_format=torch.preserve_format)\n",
        "\n",
        "\n",
        "  # calculate m_t abd v_t\n",
        "                    # `state` is the optimizer state of the parameter (tensor)\n",
        "                                           # `group` stores optimizer attributes of the parameter group\n",
        "                                                                  # `grad` is the current gradient tensor `g_t` for the parameter `θ_t−1`\n",
        "  def get_mv(self, state: Dict [str, Any], group: Dict[str, Any], grad: torch.Tensor):\n",
        "    beta1, beta2 = group['betas'] # get beta1 and beta2\n",
        "    m, v = state['exp_avg'], state['exp_avg_sq'] # get m_t-1 and v_t-1\n",
        "    m.mul_(beta1).add_(grad, alpha = 1 - beta1) # in place calculation of m_t, where m_t << beta1 * m_t-1 + (1 - beta1) * g_t\n",
        "    v.mul_(beta2).addcmul_(grad, grad, value = 1 - beta2) # in place calculation of v_t, where v_t << beta2 * v_t-1 + (1 - beta2) * g_t^2\n",
        "\n",
        "    return m, v\n",
        "\n",
        "  # get learning-rate\n",
        "  # This returns the modified learning rate based on the state. For Adam this is just specified learning rate for the parameter group, alpha\n",
        "  def get_lr(self, state: Dict[str, any], group: Dict[str, any]):\n",
        "    return group['lr']\n",
        "\n",
        "  # Do the Adam parameter update\n",
        "  def adam_update(self,\n",
        "      state: Dict[str, any], # `state` is the optimizer state of the parameter (tensor)\n",
        "      group: Dict[str, any], # `group` stores optimizer attributes of the parameter group\n",
        "      param: nn.Parameter, # `param` is the parameter tensor θ_t-1\n",
        "      m: torch.Tensor, # `m` is the uncorrected first moments m_t\n",
        "      v: torch.Tensor # `v` is the uncorrected scond moments v_t\n",
        "  ):\n",
        "  beta1, beta2 = group['betas'] # get β1 and β2\n",
        "  bias_correction1 = 1 - beta1 ** state['step'] # bias correction term for m_hat_t, (1 - β_t_1 (where `t` is superscript))\n",
        "  bias_correction2 = 1 - beta2 ** state['step'] # bias correction term for v_hat_t, (1 - β_t_2 (where `t` is superscript))\n",
        "  lr = self.get_lr(state, group) # get learning rate\n",
        "  if self.optimized_update: # whether to optimize the computation\n",
        "    denominator = v.sqrt().add_(group['eps']) # sqrt(v_t) + ϵ_hat\n",
        "    step_size = lr * math.sqrt(bias_correction2) / bias_correction # α * (sqrt(1 - β2_t (where `t` is superscript) / β1- 1_t (where `t` is superscript))\n",
        "    param.data.addcdiv_(m, denominator, value=-step_size) # θ_t << θ_t−1 - α * (sqrt(1 - β2_t (where `t` is superscript) / 1 - β1_t (where `t` is superscript)) * (m_t / sqrt(v_t) + ϵ_hat)       m_hat_t / sqrt(v_hat_t) + ϵ\n",
        "  else: # computation without optimization\n",
        "    denominator = (v.sqrt() / math.sqrt(bias_correction2)).add_(group['eps']) # sqrt(v_t) / sqrt(1-β2_t (where `t` is superscript)) + ϵ\n",
        "    step_size = lr / bias_correction1 # α / 1 - β1_t (where `t` is superscript)\n",
        "    param.data.addcdiv_(m, denominator, value=-step_size) # θ_t << θ_t−1 - α * m_hat_t / sqrt(v_hat_t) + ϵ\n",
        "\n"
      ],
      "metadata": {
        "id": "ryrRixXpoll7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yT29KIldjvlE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}